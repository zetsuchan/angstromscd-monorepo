/*************************************************************************************************

Welcome to Baml! To use this generated code, please run one of the following:

$ npm install @boundaryml/baml
$ yarn add @boundaryml/baml
$ pnpm add @boundaryml/baml

*************************************************************************************************/

// This file was generated by BAML: please do not edit it. Instead, edit the
// BAML files and re-generate this code using: baml-cli generate
// You can install baml-cli with:
//  $ npm install @boundaryml/baml
//
/* eslint-disable */
// tslint:disable
// @ts-nocheck
// biome-ignore format: autogenerated code

const fileMap = {
  
  "basic_prompts.baml": "function SimpleCompletion(prompt: string) -> string {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    {{ prompt }}\n  \"#\n}\n\nfunction AnthropicCompletion(prompt: string) -> string {\n  client \"anthropic/claude-3-haiku-20240307\"\n  prompt #\"\n    {{ prompt }}\n  \"#\n}\n\n// Tests to run in the BAML playground\n// test openai_connection {\n//   functions [SimpleCompletion]\n//   args { prompt \"Ping\" }\n// }\n// test anthropic_connection {\n//   functions [AnthropicCompletion]\n//   args { prompt \"Ping\" }\n// }\n",
  "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> LocalOllama {\n  provider \"openai-generic\"\n  options {\n    base_url \"http://localhost:11434/v1\"\n    model \"llama3.2\"\n    api_key env.OLLAMA_API_KEY\n  }\n}\n\n// Ollama Local Models\nclient<llm> OllamaQwen05B {\n  provider \"openai-generic\"\n  options {\n    model \"qwen2.5:0.5b\"\n    base_url \"http://localhost:11434/v1\"\n    api_key \"unused\"\n  }\n}\n\nclient<llm> OllamaLlama32B {\n  provider \"openai-generic\"\n  options {\n    model \"llama3.3:70b-instruct-q4_K_M\"\n    base_url env.OLLAMA_BASE_URL\n    api_key \"unused\"\n  }\n}\n\nclient<llm> OllamaLlama70B {\n  provider \"openai-generic\"\n  options {\n    model \"llama3.3:70b-instruct-q4_K_M\"\n    base_url env.OLLAMA_BASE_URL\n    api_key \"unused\"\n  }\n}\n\nclient<llm> OllamaMixtral {\n  provider \"openai-generic\"\n  options {\n    model \"mixtral:8x7b\"\n    base_url env.OLLAMA_BASE_URL\n    api_key \"unused\"\n  }\n}\n\n// Apple Foundation Models (via Swift bridge)\nclient<llm> AppleFoundation3B {\n  provider \"openai-generic\"\n  options {\n    model \"apple-foundation-3b\"\n    api_key \"local\"\n    base_url env.APPLE_BRIDGE_URL\n  }\n}\n\n// OpenRouter Models\nclient<llm> OpenRouterGemini3Pro {\n  provider \"openai-generic\"\n  options {\n    model \"google/gemini-3-pro-preview\"\n    api_key env.OPENROUTER_API_KEY\n    base_url \"https://openrouter.ai/api/v1\"\n    headers {\n      \"HTTP-Referer\" \"http://localhost:5173\"\n      \"X-Title\" \"AngstromSCD\"\n    }\n  }\n}\n\nclient<llm> OpenRouterClaudeSonnet45 {\n  provider \"openai-generic\"\n  options {\n    model \"anthropic/claude-sonnet-4.5\"\n    api_key env.OPENROUTER_API_KEY\n    base_url \"https://openrouter.ai/api/v1\"\n    headers {\n      \"HTTP-Referer\" \"http://localhost:5173\"\n      \"X-Title\" \"AngstromSCD\"\n    }\n  }\n}\n\nclient<llm> OpenRouterMiniMaxM2 {\n  provider \"openai-generic\"\n  options {\n    model \"minimax/minimax-m2\"\n    api_key env.OPENROUTER_API_KEY\n    base_url \"https://openrouter.ai/api/v1\"\n    headers {\n      \"HTTP-Referer\" \"http://localhost:5173\"\n      \"X-Title\" \"AngstromSCD\"\n    }\n  }\n}\n\nclient<llm> OpenRouterGLM46 {\n  provider \"openai-generic\"\n  options {\n    model \"z-ai/glm-4.6\"\n    api_key env.OPENROUTER_API_KEY\n    base_url \"https://openrouter.ai/api/v1\"\n    headers {\n      \"HTTP-Referer\" \"http://localhost:5173\"\n      \"X-Title\" \"AngstromSCD\"\n    }\n  }\n}\n\nclient<llm> OpenRouterGPT5 {\n  provider \"openai-generic\"\n  options {\n    model \"openai/gpt-5\"\n    api_key env.OPENROUTER_API_KEY\n    base_url \"https://openrouter.ai/api/v1\"\n    headers {\n      \"HTTP-Referer\" \"http://localhost:5173\"\n      \"X-Title\" \"AngstromSCD\"\n    }\n  }\n}\n\nclient<llm> OpenRouterGPTOSS120B {\n  provider \"openai-generic\"\n  options {\n    model \"openai/gpt-oss-120b\"\n    api_key env.OPENROUTER_API_KEY\n    base_url \"https://openrouter.ai/api/v1\"\n    headers {\n      \"HTTP-Referer\" \"http://localhost:5173\"\n      \"X-Title\" \"AngstromSCD\"\n    }\n  }\n}\n\n// LM Studio Local Models\nclient<llm> LMStudioLocal {\n  provider \"openai-generic\"\n  options {\n    model \"local-model\"  // This will be replaced with actual model loaded in LM Studio\n    api_key \"lm-studio\"\n    base_url \"http://localhost:1234/v1\"\n  }\n}\n\n// Medical-specific Ollama Models\nclient<llm> OllamaMeditron7B {\n  provider \"openai-generic\"\n  options {\n    model \"meditron\"\n    base_url \"http://localhost:11434/v1\"\n    api_key \"unused\"\n  }\n}\n\nclient<llm> OllamaMeditronLatest {\n  provider \"openai-generic\"\n  options {\n    model \"meditron:latest\"\n    base_url \"http://localhost:11434/v1\"\n    api_key \"unused\"\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
  "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"typescript\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.213.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n",
  "medical_chat.baml": "// Unified medical chat function that combines tool detection with medical expertise\n\nclass MedicalChatResponse {\n  message string @description(\"The main response to the user\")\n  requires_tools bool @description(\"Whether tools are needed\")\n  tool_calls ToolCall[] @description(\"List of tools to call\")\n  medical_context MedicalContext? @description(\"Medical context if relevant\")\n  suggestions string[] @description(\"Follow-up suggestions or questions\")\n}\n\nclass MedicalContext {\n  condition string? @description(\"Medical condition being discussed\")\n  severity string? @description(\"Severity level if applicable\")\n  treatment_options string[] @description(\"Relevant treatment options\")\n  key_considerations string[] @description(\"Important medical considerations\")\n  requires_literature bool @description(\"Whether medical literature search would be helpful\")\n}\n\n// Main medical chat function that routes to appropriate model\nfunction MedicalChat(query: string, model: string, context: string?) -> MedicalChatResponse {\n  client OpenRouterGemini3Pro\n  prompt #\"\n    You are a medical AI assistant specializing in Sickle Cell Disease (SCD) and related conditions.\n    \n    User Query: {{ query }}\n    Selected Model: {{ model }}\n    Context: {{ context }}\n    \n    Analyze the query and determine:\n    1. If any tools are needed (visualization, PubMed search, etc.)\n    2. The medical context and relevance\n    3. Appropriate response considering SCD expertise\n    \n    Available tools:\n    - E2B_CODE_INTERPRETER: For creating visualizations, charts, or data analysis\n    - PUBMED_SEARCH: For searching medical literature\n    \n    Provide a comprehensive response that includes:\n    - A helpful answer to the user's question\n    - Whether tools should be used\n    - Medical context if relevant\n    - Suggestions for follow-up\n    \n    Always remind users to consult healthcare providers for personal medical decisions.\n  \"#\n}\n\n// Ollama-compatible version for local models\nfunction MedicalChatOllama(query: string, model: string, context: string?) -> MedicalChatResponse {\n  client OllamaMeditronLatest\n  prompt #\"\n    You are Meditron, a medical AI assistant specializing in Sickle Cell Disease (SCD).\n    \n    User Query: {{ query }}\n    Context: {{ context }}\n    \n    Analyze the query and provide:\n    1. A helpful medical response\n    2. Whether visualization or literature search would help\n    3. Medical context and considerations\n    4. Follow-up suggestions\n    \n    Tools available:\n    - E2B_CODE_INTERPRETER: For charts and visualizations\n    - PUBMED_SEARCH: For medical literature\n    \n    For E2B_CODE_INTERPRETER, format arguments as:\n    {\n      \"code\": \"Python code here\",\n      \"description\": \"What this does\",\n      \"packages\": [\"matplotlib\", \"pandas\"],\n      \"expected_output\": \"chart\"\n    }\n    \n    Always remind users to consult healthcare providers for personal medical decisions.\n  \"#\n}\n\n// Function to process medical responses with appropriate model\nfunction ProcessMedicalQuery(query: string, model: string) -> MedicalChatResponse {\n  client CustomGPT4o\n  prompt #\"\n    Route this medical query to the appropriate handler based on the model.\n    \n    Query: {{ query }}\n    Model: {{ model }}\n    \n    If model is \"meditron:latest\" or starts with \"llama\", \"qwen\", or \"mixtral\", use Ollama.\n    Otherwise use cloud models (GPT-4, Claude).\n    \n    Process the query with appropriate medical expertise.\n  \"#\n}\n\n// Test cases for medical chat\ntest medical_chat_visualization {\n  functions [MedicalChat]\n  args {\n    query \"Show me a chart of VOE frequency patterns over 12 months\"\n    model \"gpt-4o\"\n  }\n}\n\ntest medical_chat_literature {\n  functions [MedicalChat]\n  args {\n    query \"What are the latest treatments for pediatric SCD patients?\"\n    model \"meditron:latest\"\n  }\n}\n\ntest medical_chat_general {\n  functions [MedicalChatOllama]\n  args {\n    query \"Explain hydroxyurea dosing for SCD\"\n    model \"meditron:latest\"\n  }\n}",
  "medical_researcher.baml": "// Medical Research Assistant for Sickle Cell Disease\n\nclass MedicalInsight {\n  summary string\n  key_findings string[]\n  citations Citation[]\n  recommendations string[]\n  confidence_level string @description(\"high, medium, or low\")\n}\n\nclass Citation {\n  title string\n  authors string[]\n  journal string\n  year int\n  pmid string?\n  doi string?\n  relevance_score float @description(\"0.0 to 1.0\")\n}\n\nfunction MedicalResearcher(query: string) -> MedicalInsight {\n  client OpenRouterGemini3Pro\n  prompt #\"\n    You are a medical research assistant specializing in Sickle Cell Disease (SCD). \n    Analyze the following query and provide comprehensive medical insights.\n\n    Query: {{ query }}\n\n    Provide a response with:\n    1. A concise summary of the medical information\n    2. Key findings relevant to the query\n    3. Relevant citations from medical literature (if applicable)\n    4. Clinical recommendations based on current guidelines\n    5. Your confidence level in the response (high, medium, or low)\n\n    Focus on evidence-based information and clearly distinguish between established facts \n    and areas requiring further research.\n  \"#\n}\n\n// Test function for the playground\ntest medical_query {\n  functions [MedicalResearcher]\n  args {\n    query \"What are the latest treatments for VOE in pediatric SCD patients?\"\n  }\n}",
  "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
  "simple_chat.baml": "// Simple chat function for conversational responses\n\nfunction SimpleChat(query: string) -> string {\n  client OpenRouterGemini3Pro\n  prompt #\"\n    You are MedLab AI, a medical assistant specializing in Sickle Cell Disease (SCD).\n\n    Respond to the following query in a helpful, conversational manner:\n    {{ query }}\n\n    Provide clear, evidence-based information when discussing medical topics.\n    Always remind users to consult healthcare providers for personal medical decisions.\n  \"#\n}\n\n// Test function\ntest simple_chat_test {\n  functions [SimpleChat]\n  args {\n    query \"What is sickle cell disease?\"\n  }\n}",
  "templates/literature-search.baml": "function LiteratureSearch(research_query: string, medical_domain: string, time_period: string) -> string {\n  client LocalOllama\n  prompt #\"\n    You are a medical research assistant specialized in literature analysis and research synthesis.\n    \n    Research Query: {{ research_query }}\n    Medical Domain: {{ medical_domain }}\n    Time Period: {{ time_period }}\n    \n    Please provide a comprehensive literature search analysis including:\n    1. Key research areas and methodologies relevant to the query\n    2. Important findings and clinical implications\n    3. Current gaps in research and emerging trends\n    4. Recommended search terms and databases\n    5. Quality assessment criteria for evaluating studies\n    6. Synthesis of evidence levels and recommendations\n    \n    Focus on evidence-based medicine principles and provide structured insights that can guide clinical decision-making.\n    \n    Literature Analysis:\n  \"#\n}\n\nfunction ResearchSynthesis(papers: string[], research_question: string) -> string {\n  client LocalOllama\n  prompt #\"\n    You are a research synthesis expert specializing in medical literature review.\n    \n    Research Question: {{ research_question }}\n    Papers to Analyze: {{ papers }}\n    \n    Please provide a systematic synthesis including:\n    1. Study design and methodology comparison\n    2. Population characteristics and sample sizes\n    3. Key findings and effect sizes\n    4. Limitations and potential biases\n    5. Clinical significance and applicability\n    6. Recommendations for future research\n    \n    Synthesis Report:\n  \"#\n} ",
  "templates/medical-analysis.baml": "function MedicalAnalysis(patient_data: string, symptoms: string, medical_history: string) -> string {\n  client LocalOllama\n  prompt #\"\n    You are a medical AI assistant specialized in analyzing patient data and symptoms.\n    \n    Patient Data: {{ patient_data }}\n    Current Symptoms: {{ symptoms }}\n    Medical History: {{ medical_history }}\n    \n    Please provide a comprehensive medical analysis including:\n    1. Symptom assessment and potential correlations\n    2. Risk factors based on medical history\n    3. Recommended diagnostic tests or procedures\n    4. Potential differential diagnoses to consider\n    5. Suggested monitoring parameters\n    \n    Important: This analysis is for informational purposes only and should not replace professional medical consultation.\n    \n    Analysis:\n  \"#\n} ",
  "templates/risk-modeling.baml": "function RiskModeling(patient_profile: string, risk_factors: string[], outcome_target: string) -> string {\n  client LocalOllama\n  prompt #\"\n    You are a medical risk assessment specialist with expertise in predictive modeling and clinical risk stratification.\n    \n    Patient Profile: {{ patient_profile }}\n    Risk Factors: {{ risk_factors }}\n    Target Outcome: {{ outcome_target }}\n    \n    Please provide a comprehensive risk assessment including:\n    1. Risk factor analysis and weighting\n    2. Interaction effects between risk factors\n    3. Risk stratification (low, moderate, high risk)\n    4. Probability estimates and confidence intervals\n    5. Modifiable vs non-modifiable risk factors\n    6. Risk mitigation strategies and interventions\n    7. Monitoring recommendations and follow-up schedule\n    \n    Use evidence-based risk scoring systems where applicable and provide clear clinical actionability.\n    \n    Risk Assessment:\n  \"#\n}\n\nfunction PopulationRiskAnalysis(population_data: string, demographic_factors: string, environmental_factors: string) -> string {\n  client LocalOllama\n  prompt #\"\n    You are an epidemiologist specializing in population health risk analysis and public health modeling.\n    \n    Population Data: {{ population_data }}\n    Demographic Factors: {{ demographic_factors }}\n    Environmental Factors: {{ environmental_factors }}\n    \n    Please provide a population-level risk analysis including:\n    1. Prevalence and incidence patterns\n    2. Demographic risk stratification\n    3. Environmental and social determinants impact\n    4. Geographic and temporal risk variations\n    5. Population attributable risk calculations\n    6. Public health intervention priorities\n    7. Resource allocation recommendations\n    \n    Population Risk Analysis:\n  \"#\n}\n\nfunction ClinicalDecisionSupport(clinical_scenario: string, patient_data: string, treatment_options: string[]) -> string {\n  client LocalOllama\n  prompt #\"\n    You are a clinical decision support system specializing in evidence-based treatment recommendations.\n    \n    Clinical Scenario: {{ clinical_scenario }}\n    Patient Data: {{ patient_data }}\n    Treatment Options: {{ treatment_options }}\n    \n    Please provide clinical decision support including:\n    1. Risk-benefit analysis for each treatment option\n    2. Patient-specific contraindications and considerations\n    3. Evidence quality and recommendation strength\n    4. Shared decision-making talking points\n    5. Monitoring parameters for chosen interventions\n    6. Alternative approaches if first-line fails\n    \n    Clinical Recommendation:\n  \"#\n} ",
  "tools.baml": "// Tool definitions for medical research assistant\n\nenum ToolType {\n  E2B_CODE_INTERPRETER\n  PUBMED_SEARCH\n  PERPLEXITY_SEARCH\n  EXA_SEARCH\n}\n\nclass Tool {\n  type ToolType\n  name string\n  description string\n}\n\nclass ToolCall {\n  tool ToolType\n  arguments string @description(\"JSON string of tool arguments\")\n  reasoning string @description(\"Why this tool is being called\")\n}\n\nclass E2BCodeRequest {\n  code string @description(\"Python code to execute\")\n  description string @description(\"What the code does\")\n  packages string[] @description(\"Python packages to install\")\n  expected_output string @description(\"What output format is expected\")\n}\n\nclass ChatResponseWithTools {\n  message string @description(\"Natural language response to the user\")\n  requires_tools bool @description(\"Whether tools are needed to answer\")\n  tool_calls ToolCall[] @description(\"List of tools to call\")\n  citations Citation[]\n}\n\nfunction DetermineToolUsage(query: string, context: string?) -> ChatResponseWithTools {\n  client CustomGPT4o\n  prompt #\"\n    You are a medical research assistant that can use various tools to provide comprehensive answers.\n    \n    Available tools:\n    1. E2B_CODE_INTERPRETER - For creating visualizations, charts, graphs, or running data analysis\n    2. PUBMED_SEARCH - For searching medical literature  \n    3. PERPLEXITY_SEARCH - For current medical news and web search\n    4. EXA_SEARCH - For deep medical research papers\n    \n    Query: {{ query }}\n    Context: {{ context }}\n    \n    Analyze if the query requires any tools, especially:\n    - If the user asks for charts, graphs, plots, visualizations, or data analysis -> use E2B_CODE_INTERPRETER\n    - If the user asks about medical literature or research -> use PUBMED_SEARCH\n    - If the user asks about recent news or current information -> use PERPLEXITY_SEARCH\n    - If the user needs deep research analysis -> use EXA_SEARCH\n    \n    For E2B_CODE_INTERPRETER, the arguments should be a JSON string containing:\n    {\n      \"code\": \"Python code here\",\n      \"description\": \"What this code does\",\n      \"packages\": [\"matplotlib\", \"pandas\", etc],\n      \"expected_output\": \"chart\" or \"data\" or \"analysis\"\n    }\n    \n    Provide:\n    1. A natural language message to the user\n    2. Whether tools are required\n    3. Which tools to call with their arguments\n    4. Any immediate citations if available\n  \"#\n}\n\nfunction GenerateVisualizationCode(request: string, data_context: string?) -> E2BCodeRequest {\n  client CustomGPT4o\n  prompt #\"\n    You are an expert at creating medical data visualizations using Python.\n    \n    User request: {{ request }}\n    Data context: {{ data_context }}\n    \n    Generate Python code that:\n    1. Creates the requested visualization\n    2. Uses matplotlib, seaborn, or plotly as appropriate\n    3. Includes proper labels, titles, and legends\n    4. Handles medical data appropriately\n    5. Saves the plot as output\n    \n    Focus on medical accuracy and clarity. Use appropriate color schemes for accessibility.\n    Include any necessary data generation if no specific data is provided.\n    \n    Always end with plt.show() or fig.show() to display the visualization.\n  \"#\n}\n\n// Test for tool detection\ntest tool_detection {\n  functions [DetermineToolUsage]\n  args {\n    query \"Create a bar chart showing the effectiveness of different SCD treatments\"\n  }\n}\n\n// Test for visualization generation\ntest visualization_generation {\n  functions [GenerateVisualizationCode]\n  args {\n    request \"Create a line graph showing VOE frequency over 12 months\"\n    data_context \"Patient has 3-5 VOE episodes per month on average\"\n  }\n}\n\n// Ollama-compatible visualization code generator\nfunction GenerateVisualizationCodeOllama(request: string, data_context: string?) -> E2BCodeRequest {\n  client OllamaQwen05B\n  prompt #\"\n    You are an expert at creating medical data visualizations using Python.\n    \n    User request: {{ request }}\n    Data context: {{ data_context }}\n    \n    Generate Python code that:\n    1. Creates the requested visualization\n    2. Uses matplotlib, seaborn, or plotly as appropriate\n    3. Includes proper labels, titles, and legends\n    4. Handles medical data appropriately\n    5. Saves the plot as output\n    \n    Focus on medical accuracy and clarity. Use appropriate color schemes for accessibility.\n    Include any necessary data generation if no specific data is provided.\n    \n    Always end with plt.show() or fig.show() to display the visualization.\n  \"#\n}\n\n// Ollama-compatible tool detection function\nfunction DetermineToolUsageOllama(query: string, context: string?) -> ChatResponseWithTools {\n  client OllamaMeditronLatest\n  prompt #\"\n    You are a medical research assistant that can use various tools to provide comprehensive answers.\n    \n    Available tools:\n    1. E2B_CODE_INTERPRETER - For creating visualizations, charts, graphs, or running data analysis\n    2. PUBMED_SEARCH - For searching medical literature  \n    3. PERPLEXITY_SEARCH - For current medical news and web search\n    4. EXA_SEARCH - For deep medical research papers\n    \n    Query: {{ query }}\n    Context: {{ context }}\n    \n    Analyze if the query requires any tools, especially:\n    - If the user asks for charts, graphs, plots, visualizations, or data analysis -> use E2B_CODE_INTERPRETER\n    - If the user asks about medical literature or research -> use PUBMED_SEARCH\n    - If the user asks about recent news or current information -> use PERPLEXITY_SEARCH\n    - If the user needs deep research analysis -> use EXA_SEARCH\n    \n    For E2B_CODE_INTERPRETER, the arguments should be a JSON string containing:\n    {\n      \"code\": \"Python code here\",\n      \"description\": \"What this code does\",\n      \"packages\": [\"matplotlib\", \"pandas\", etc],\n      \"expected_output\": \"chart\" or \"data\" or \"analysis\"\n    }\n    \n    Provide:\n    1. A natural language message to the user\n    2. Whether tools are required\n    3. Which tools to call with their arguments\n    4. Any immediate citations if available\n  \"#\n}",
}
export const getBamlFiles = () => {
    return fileMap;
}