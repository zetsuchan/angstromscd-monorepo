/*************************************************************************************************

Welcome to Baml! To use this generated code, please run one of the following:

$ npm install @boundaryml/baml
$ yarn add @boundaryml/baml
$ pnpm add @boundaryml/baml

*************************************************************************************************/

// This file was generated by BAML: do not edit it. Instead, edit the BAML
// files and re-generate this code.
//
/* eslint-disable */
// tslint:disable
// @ts-nocheck
// biome-ignore format: autogenerated code
const fileMap = {
  
  "basic_prompts.baml": "function SimpleCompletion(prompt: string) -> string {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    {{ prompt }}\n  \"#\n}\n\nfunction AnthropicCompletion(prompt: string) -> string {\n  client \"anthropic/claude-3-haiku-20240307\"\n  prompt #\"\n    {{ prompt }}\n  \"#\n}\n\n// Tests to run in the BAML playground\n// test openai_connection {\n//   functions [SimpleCompletion]\n//   args { prompt \"Ping\" }\n// }\n// test anthropic_connection {\n//   functions [AnthropicCompletion]\n//   args { prompt \"Ping\" }\n// }\n",
  "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> LocalOllama {\n  provider openai\n  options {\n    base_url \"http://localhost:11434/v1\"\n    model \"llama3.2\"\n    api_key \"ollama\"\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
  "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"typescript\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.89.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n",
  "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
  "templates/literature-search.baml": "function LiteratureSearch(research_query: string, medical_domain: string, time_period: string) -> string {\n  client LocalOllama\n  prompt #\"\n    You are a medical research assistant specialized in literature analysis and research synthesis.\n    \n    Research Query: {{ research_query }}\n    Medical Domain: {{ medical_domain }}\n    Time Period: {{ time_period }}\n    \n    Please provide a comprehensive literature search analysis including:\n    1. Key research areas and methodologies relevant to the query\n    2. Important findings and clinical implications\n    3. Current gaps in research and emerging trends\n    4. Recommended search terms and databases\n    5. Quality assessment criteria for evaluating studies\n    6. Synthesis of evidence levels and recommendations\n    \n    Focus on evidence-based medicine principles and provide structured insights that can guide clinical decision-making.\n    \n    Literature Analysis:\n  \"#\n}\n\nfunction ResearchSynthesis(papers: string[], research_question: string) -> string {\n  client LocalOllama\n  prompt #\"\n    You are a research synthesis expert specializing in medical literature review.\n    \n    Research Question: {{ research_question }}\n    Papers to Analyze: {{ papers }}\n    \n    Please provide a systematic synthesis including:\n    1. Study design and methodology comparison\n    2. Population characteristics and sample sizes\n    3. Key findings and effect sizes\n    4. Limitations and potential biases\n    5. Clinical significance and applicability\n    6. Recommendations for future research\n    \n    Synthesis Report:\n  \"#\n} ",
  "templates/medical-analysis.baml": "function MedicalAnalysis(patient_data: string, symptoms: string, medical_history: string) -> string {\n  client LocalOllama\n  prompt #\"\n    You are a medical AI assistant specialized in analyzing patient data and symptoms.\n    \n    Patient Data: {{ patient_data }}\n    Current Symptoms: {{ symptoms }}\n    Medical History: {{ medical_history }}\n    \n    Please provide a comprehensive medical analysis including:\n    1. Symptom assessment and potential correlations\n    2. Risk factors based on medical history\n    3. Recommended diagnostic tests or procedures\n    4. Potential differential diagnoses to consider\n    5. Suggested monitoring parameters\n    \n    Important: This analysis is for informational purposes only and should not replace professional medical consultation.\n    \n    Analysis:\n  \"#\n} ",
  "templates/risk-modeling.baml": "function RiskModeling(patient_profile: string, risk_factors: string[], outcome_target: string) -> string {\n  client LocalOllama\n  prompt #\"\n    You are a medical risk assessment specialist with expertise in predictive modeling and clinical risk stratification.\n    \n    Patient Profile: {{ patient_profile }}\n    Risk Factors: {{ risk_factors }}\n    Target Outcome: {{ outcome_target }}\n    \n    Please provide a comprehensive risk assessment including:\n    1. Risk factor analysis and weighting\n    2. Interaction effects between risk factors\n    3. Risk stratification (low, moderate, high risk)\n    4. Probability estimates and confidence intervals\n    5. Modifiable vs non-modifiable risk factors\n    6. Risk mitigation strategies and interventions\n    7. Monitoring recommendations and follow-up schedule\n    \n    Use evidence-based risk scoring systems where applicable and provide clear clinical actionability.\n    \n    Risk Assessment:\n  \"#\n}\n\nfunction PopulationRiskAnalysis(population_data: string, demographic_factors: string, environmental_factors: string) -> string {\n  client LocalOllama\n  prompt #\"\n    You are an epidemiologist specializing in population health risk analysis and public health modeling.\n    \n    Population Data: {{ population_data }}\n    Demographic Factors: {{ demographic_factors }}\n    Environmental Factors: {{ environmental_factors }}\n    \n    Please provide a population-level risk analysis including:\n    1. Prevalence and incidence patterns\n    2. Demographic risk stratification\n    3. Environmental and social determinants impact\n    4. Geographic and temporal risk variations\n    5. Population attributable risk calculations\n    6. Public health intervention priorities\n    7. Resource allocation recommendations\n    \n    Population Risk Analysis:\n  \"#\n}\n\nfunction ClinicalDecisionSupport(clinical_scenario: string, patient_data: string, treatment_options: string[]) -> string {\n  client LocalOllama\n  prompt #\"\n    You are a clinical decision support system specializing in evidence-based treatment recommendations.\n    \n    Clinical Scenario: {{ clinical_scenario }}\n    Patient Data: {{ patient_data }}\n    Treatment Options: {{ treatment_options }}\n    \n    Please provide clinical decision support including:\n    1. Risk-benefit analysis for each treatment option\n    2. Patient-specific contraindications and considerations\n    3. Evidence quality and recommendation strength\n    4. Shared decision-making talking points\n    5. Monitoring parameters for chosen interventions\n    6. Alternative approaches if first-line fails\n    \n    Clinical Recommendation:\n  \"#\n} ",
}
export const getBamlFiles = () => {
    return fileMap;
}