import { type ChatRequest, type ModelName, ModelProviderMap } from "../types";
import {
	generateConversationalResponse,
	runAnthropicChat,
	runAppleFoundationChat,
	runLMStudioChat,
	runOllamaChat,
	runOpenAIChat,
	runOpenRouterChat,
	testAnthropicConnection,
	testAppleBridgeConnection,
	testLMStudioConnection,
	testOllamaConnection,
	testOpenAIConnection,
	testOpenRouterConnection,
} from "./baml-service";

/**
 * Routes a chat request to the appropriate AI model provider and returns the generated response.
 *
 * Determines the provider based on the request's explicit provider field or by mapping the model name. Defaults to OpenAI if no provider is specified. Supports OpenAI, Anthropic, Ollama, and Apple Foundation providers.
 *
 * @param request - The chat request containing message, model, provider, temperature, and maxTokens.
 * @returns The response generated by the selected provider's chat model.
 *
 * @throws {Error} If the specified provider is not supported.
 */
export async function routeModelChat(request: ChatRequest): Promise<string> {
	const { message, model, provider, temperature, maxTokens } = request;

	// Determine provider from model name or explicit provider
	let selectedProvider = provider;
	if (!selectedProvider && model) {
		selectedProvider = ModelProviderMap[model as ModelName] as typeof provider;
	}

	// Default to OpenAI if no provider specified
	if (!selectedProvider) {
		selectedProvider = "openai";
	}

	// Use the unified conversational response handler with dynamic model selection
	return generateConversationalResponse(
		message,
		[],
		selectedProvider,
		model || "gpt-4o-mini",
	);
}

/**
 * Returns a list of available chat models and their associated providers.
 *
 * @returns An array of objects, each containing the model name, provider, and an `available` flag set to true.
 *
 * @remark The `available` flag is always true and does not reflect real-time provider availability.
 */
export function getAvailableModels() {
	return Object.entries(ModelProviderMap).map(([model, provider]) => ({
		model,
		provider,
		available: true, // In production, check actual availability
	}));
}

/**
 * Checks connectivity to all supported AI model providers.
 *
 * @returns An object indicating the connection status for each provider.
 */
export async function checkAllProviders() {
	const [openai, anthropic, openrouter, lmstudio, ollama, apple] =
		await Promise.all([
			testOpenAIConnection(),
			testAnthropicConnection(),
			testOpenRouterConnection(),
			testLMStudioConnection(),
			testOllamaConnection(),
			testAppleBridgeConnection(),
		]);

	return {
		openai,
		anthropic,
		openrouter,
		lmstudio,
		ollama,
		apple,
	};
}
